# -*- coding: utf-8 -*-
"""atividade 05-wikipedia_streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y7iJ8UjkU0MTiYF0riL288zLFetuE2I7

# Building a Network from Wikipedia Pages

* [Pipeline to extract a network from Wikipedia.](https://github.com/VictorNGomes/Directed_Networks_From_Wikipedia_Pages/blob/main/notebooks/Pipeline.ipynb )

* [Week 07 Video 01 - Building a Network from Wikipedia Pages](https://youtu.be/EhOhCdkgmxs)

* [Week 07 Video 02 - Snowballing process to collect Wikipedia pages](https://youtu.be/RdU6Kj9N_ow)
"""

!pip install wikipedia


from operator import itemgetter
import networkx as nx
import wikipedia
import matplotlib.pyplot as plt
import seaborn as sns


nx.__version__

#!pip install scipy==1.8

SEED = "Conspiracy theory".title()
STOPS = ("International Standard Serial Number",
         "International Standard Book Number",
         "National Diet Library",
         "International Standard Name Identifier",
         "International Standard Book Number (Identifier)",
         "Pubmed Identifier",
         "Pubmed Central",
         "Digital Object Identifier",
         "Arxiv",
         "Proc Natl Acad Sci Usa",
         "Bibcode",
         "Library Of Congress Control Number",
         "Jstor",
         "Doi (Identifier)",
         "Isbn (Identifier)",
         "Pmid (Identifier)",
         "Arxiv (Identifier)",
         "Bibcode (Identifier)",
         "Alchemy"
         )

todo_lst = [(0, SEED)] # The SEED is in the layer 0
todo_set = set(SEED) # The SEED itself
done_set = set() # Nothing is done yet

g = nx.DiGraph()
layer, page = todo_lst[0]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# while layer < 2 and len(g.nodes) < 5000:
#     # Remove the name page of the current page from the todo_lst,
#     # and add it to the set of processed pages.
#     # If the script encounters this page again, it will skip over it.
#     del todo_lst[0]
#     done_set.add(page)
# 
#     # Show progress
#     print(layer, page)
# 
#     # Attempt to download the selected page.
#     try:
#         wiki = wikipedia.page(page)
#     except:
#         print("Could not load", page)
#         if todo_lst:
#             layer, page = todo_lst[0]
#         continue
# 
#     for link in wiki.links:
#         link = link.title()
#         if link not in STOPS and not link.startswith("List Of"):
#             if link not in todo_set and link not in done_set:
#                 # Verifica se atingiu o limite de nÃ³s
#                 if len(g.nodes) >= 2000:
#                     break
#                 todo_lst.append((layer + 1, link))
#                 todo_set.add(link)
#             g.add_edge(page, link)
# 
#     if todo_lst:
#         layer, page = todo_lst[0]
#     else:
#         break
# 
# print("{} nodes, {} edges".format(len(g), nx.number_of_edges(g)))
#

print("{} nodes, {} edges".format(len(g), nx.number_of_edges(g)))

page = 'Conspiracy theory'.title()
wiki = wikipedia.page(page)
len(wiki.links)

# make a copy of raw graph
original = g.copy()

# remove self loops
g.remove_edges_from(nx.selfloop_edges(g))

# Explicitly remove the 'contraction' attribute from all nodes
# before starting the contraction process.
for node in g.nodes():
    if 'contraction' in g.nodes[node]:
        del g.nodes[node]['contraction']


# identify duplicates like that: 'network' and 'networks'
duplicates = [(node, node + "s")
              for node in g if node + "s" in g
             ]

for dup in duplicates:
  # *dup is a technique named 'unpacking'
  # contracted_nodes will handle the 'contraction' attribute internally
  g = nx.contracted_nodes(g, *dup, self_loops=False)

print(duplicates)

duplicates = [(x, y) for x, y in
              [(node, node.replace("-", " ")) for node in g]
                if x != y and y in g]
print(duplicates)

for dup in duplicates:
  # contracted_nodes will handle the 'contraction' attribute internally
  g = nx.contracted_nodes(g, *dup, self_loops=False)

# The 'contraction' attribute added by nx.contracted_nodes is a dictionary.
# GraphML does not support dictionary attributes. We need to convert it or remove it.
# A common way to handle this is to convert the dictionary into a string representation.
# Alternatively, if the 'contraction' attribute is not needed for the GraphML export,
# you can remove it from the nodes.

# Let's convert the 'contraction' attribute to a string for GraphML compatibility
for node, data in g.nodes(data=True):
    if 'contraction' in data and isinstance(data['contraction'], dict):
        data['contraction'] = str(data['contraction'])
    # If you don't need the contraction info in GraphML, you can remove it instead:
    # if 'contraction' in data:
    #     del data['contraction']

# We don't need to set a 'contraction' attribute for edges created by contraction
# because self_loops=False is used, which handles edge contraction appropriately.
# If you were dealing with edge contraction and its 'contraction' attribute, you'd
# need similar handling as the node attribute.

print("{} nodes, {} edges".format(len(g), nx.number_of_edges(g)))

degree = [drg for node , drg in g.degree()]
print(f'Max degree : {max(degree)}' )
print(f'Min degree : {min(degree)}' )

sns.histplot(data=degree, kde=True)

#filter nodes with degree greater than 2
#core decompo = 3
core = [node for node, drg in dict(g.degree()).items() if drg > 2 ]
degrees = [drg for node, drg in dict(g.degree()).items() if drg > 2 ]

subgraf_g = nx.subgraph(g,core)
print(f'Nodes: {len(subgraf_g)} edges: {nx.number_of_edges(subgraf_g)}')

# Before writing to GraphML, ensure no dictionary attributes remain on nodes or edges
for node, data in subgraf_g.nodes(data=True):
    attributes_to_convert = [key for key, value in data.items() if isinstance(value, dict)]
    for key in attributes_to_convert:
        data[key] = str(data[key]) # Convert dictionary to string

for u, v, data in subgraf_g.edges(data=True):
    attributes_to_convert = [key for key, value in data.items() if isinstance(value, dict)]
    for key in attributes_to_convert:
         data[key] = str(data[key]) # Convert dictionary to string


nx.write_graphml(subgraf_g, "network_analysis.graphml")

print("Nodes removed: {:.2f}%".format(100*(1 - len(subgraf_g)/len(g))))
print("Edges removed: {:.2f}%".format(100*(1 - nx.number_of_edges(subgraf_g)/nx.number_of_edges(g))))
print("Original Average of edges by node: {:.2f}".format(nx.number_of_edges(g)/len(g)))
print("Subgraph Average of edges by node: {:.2f}".format(nx.number_of_edges(subgraf_g)/len(subgraf_g)))

import numpy as np
from collections import  Counter

Counter(degrees)

top_indegree = sorted(dict(subgraf_g.in_degree()).items(),
                      reverse=True, key=itemgetter(1))[:100]

print("\n".join(map(lambda t: "{} {}".format(*reversed(t)), top_indegree)))

sns.histplot(data=degrees,kde = True,bins=np.histogram_bin_edges(degrees,50,range = (0,1000)))

"""# degree_centrality"""

#the degree centrality of network(g)
fig, ax = plt.subplots(1,1,figsize=(10,8))

# layout position
pos = nx.spring_layout(subgraf_g,seed=8375,k=0.2)
# color of nodes
color = list(dict(nx.degree_centrality(subgraf_g)).values())

# draw edges
nx.draw_networkx_edges(subgraf_g,
                       pos=pos,
                       alpha=0.4, ax=ax)

# draw nodes
nodes = nx.draw_networkx_nodes(subgraf_g,
                 pos=pos,
                 node_color=color,
                 cmap=plt.cm.jet,
                 ax=ax)

# draw labels
nx.draw_networkx_labels(subgraf_g, pos=pos,
                        font_color='white',
                        font_size = 5,
                        ax=ax)


plt.axis("off")
plt.colorbar(nodes)
plt.savefig('degree_centrality.png', transparent=True,dpi=300)
plt.show()

"""# - Closeness Centrality"""

nx.closeness_centrality(g)

# the closeness centrality of network(g)
fig, ax = plt.subplots(1,1,figsize=(10,8))

# layout position
pos = nx.spring_layout(subgraf_g,seed=123456789,k=0.3)
# color of nodes
color = list(dict(nx.closeness_centrality(subgraf_g)).values())

# draw edges
nx.draw_networkx_edges(subgraf_g,
                       pos=pos,
                       alpha=0.4, ax=ax)

# draw nodes
nodes = nx.draw_networkx_nodes(subgraf_g,
                 pos=pos,
                 node_color=color,
                 cmap=plt.cm.jet,
                 ax=ax)

# draw labels
nx.draw_networkx_labels(subgraf_g, pos=pos,
                        font_color='white', ax=ax,font_size = 4)


plt.axis("off")
plt.colorbar(nodes)
plt.savefig('closeness_centrality.png', transparent=True,dpi=600)
plt.show()

# the eigenvector centrality of network(g)
fig, ax = plt.subplots(1,1,figsize=(10,8))

# layout position
pos = nx.spring_layout(subgraf_g,seed=123456789,k=0.3)
# color of nodes
color = list(dict(nx.betweenness_centrality(subgraf_g)).values())

# draw edges
nx.draw_networkx_edges(subgraf_g,
                       pos=pos,
                       alpha=0.4, ax=ax)

# draw nodes
nodes = nx.draw_networkx_nodes(subgraf_g,
                 pos=pos,
                 node_color=color,
                 cmap=plt.cm.jet,
                 ax=ax)

# draw labels
nx.draw_networkx_labels(subgraf_g, pos=pos,
                        font_color='white', ax=ax,font_size = 4)


plt.axis("off")
plt.colorbar(nodes)
plt.savefig('betweenness_centrality.png', transparent=True,dpi=600)
plt.show()

# the eigenvector centrality of network(g)
fig, ax = plt.subplots(1,1,figsize=(10,8))

# layout position
pos = nx.spring_layout(g,seed=123456789,k=0.3)
# color of nodes
color = list(dict(nx.eigenvector_centrality(subgraf_g)).values())

# draw edges
nx.draw_networkx_edges(subgraf_g,
                       pos=pos,
                       alpha=0.4, ax=ax)

# draw nodes
nodes = nx.draw_networkx_nodes(subgraf_g,
                 pos=pos,
                 node_color=color,
                 cmap=plt.cm.jet,
                 ax=ax)

# draw labels
nx.draw_networkx_labels(subgraf_g, pos=pos,
                        font_color='white', ax=ax,font_size = 4)


plt.axis("off")
plt.colorbar(nodes)
plt.savefig('eigenvector_centrality.png', transparent=True,dpi=600)
plt.show()

import pandas as pd

bc = pd.Series(nx.betweenness_centrality(subgraf_g))
dc = pd.Series(nx.degree_centrality(subgraf_g))
ec = pd.Series(nx.eigenvector_centrality(subgraf_g))
cc = pd.Series(nx.closeness_centrality(subgraf_g))

df = pd.DataFrame.from_dict({"Betweenness": bc,
                            "Degree": dc,
                            "EigenVector": ec,
                            "Closeness": cc})
df.reset_index(inplace=True,drop=True)
df.head()

fig = sns.PairGrid(df)
fig.map_upper(sns.scatterplot)
fig.map_lower(sns.kdeplot, cmap="Reds_r")
fig.map_diag(sns.kdeplot, lw=2, legend=False)


plt.savefig('all.png', transparent=True,dpi=800,bbox_inches="tight")
plt.show()

# How many k-cores does this network have?
set([v for k,v in nx.core_number(subgraf_g).items()])

import matplotlib.patches as mpatches

# the degree of network(g2)
fig, ax = plt.subplots(1,1,figsize=(10,8))

# Find k-cores
g2_core_9 = nx.k_shell(subgraf_g, 9)
g2_core_10 = nx.k_core(subgraf_g, 10)

# layout position
pos = nx.spring_layout(subgraf_g,seed=123456789,k=0.3)

# draw edges
nx.draw_networkx_edges(subgraf_g,
                       pos=pos,
                       alpha=0.4, ax=ax)

# draw nodes
nodes = nx.draw_networkx_nodes(subgraf_g,
                 pos=pos,
                 node_color="#333333")

# draw nodes
nodes = nx.draw_networkx_nodes(g2_core_9,
                 pos=pos,
                 node_color="blue")

nodes = nx.draw_networkx_nodes(g2_core_10,
                 pos=pos,
                 node_color="red")

# static legend
red_patch = mpatches.Patch(color='red', label='10-core')
blue_patch = mpatches.Patch(color='blue', label='9-shell')
plt.legend(handles=[red_patch,blue_patch])

plt.axis("off")
plt.savefig('k-core_sociopatterns.png', transparent=True,dpi=600)
plt.show()

plt.style.use("default")
# degree sequence
degree_sequence = sorted([d for n, d in subgraf_g.degree()], reverse=True)

fig, ax = plt.subplots(1,2,figsize=(8,6))

# all_data has information about degree_sequence and the width of each bin
all_data = ax[0].hist(degree_sequence,bins=7)
ax[1].hist(degree_sequence,bins=7,density=True)

ax[0].set_title("Degree Histogram")
ax[0].set_ylabel("Count")
ax[0].set_xlabel("Degree")

ax[1].set_title("Probability Density Function")
ax[1].set_ylabel("Probability")
ax[1].set_xlabel("Degree")

plt.tight_layout()
plt.show()

plt.style.use("fivethirtyeight")
#plt.style.use("default")

fig, ax = plt.subplots(1,1,figsize=(10,8))

sns.histplot(degree_sequence,bins=7,label="Count",ax=ax)
ax2 = ax.twinx()
sns.kdeplot(degree_sequence,color='r',label="Probability Density Function (PDF)",ax=ax2)

# ask matplotlib for the plotted objects and their labels
lines, labels = ax.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax2.legend(lines + lines2, labels + labels2, loc=0)

ax.grid(False)
ax2.grid(False)
ax.set_xlabel("Degree")
ax2.set_ylabel("Probability")

plt.savefig('probability_density_function.png', transparent=True,dpi=600,bbox_inches="tight")
plt.show()

plt.style.use("ggplot")

# Another way to visualize the cumulative distribution
def ecdf(data):
    return np.sort(data), np.arange(1, len(data) + 1) / len(data)

def ecdf_degree(G):
    """ECDF of degree."""
    num_neighbors = [len(list(G.neighbors(n))) for n in G.nodes()]
    x, y = ecdf(num_neighbors)
    plt.scatter(x, y)
    plt.xlabel("degree")
    plt.ylabel("cumulative fraction")


#
# insighs
# cdf(1) - almore 40% of vertices has degree 1
# cdf(2) P(degree <= 2) - 50% of vertices has at least degree 2 or less
# cdf(6) P(degree <= 6) - more than 95% of vertices has at least degree 6 or less
ecdf_degree(subgraf_g)